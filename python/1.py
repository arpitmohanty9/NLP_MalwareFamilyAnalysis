import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import sys

# IN = re.compile(r'.*\bin\b(?!\b.+ing)')
# for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):
#     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):
#             print(nltk.sem.rtuple(rel))
#
# exit()

if len(sys.argv) != 2:
    print("NOPE")
    exit()

sentences = (sys.argv)[1]
print(sentences)

a = word_tokenize(sentences)
b = nltk.pos_tag(a)

NNP = [item[0]   for item in b if (item[1] == "NNP" and not item[0] == "'" and len(item[0]) > 1)]
print(NNP)

fd = FreqDist(word.lower() for word in NNP)


# print(a)
print(b)

for word, frequency in fd.most_common(10):
    print(u'{};{}'.format(word, frequency))