#import requests
import urllib.request
from selenium import webdriver
from selenium import webdriver
from bs4 import BeautifulSoup
import csv
import sys
symantec_url = 'https://www.symantec.com'
search_url = 'https://www.symantec.com/connect/search/'


def get_blog_lists(malware_name):
    driver = webdriver.Firefox()
    driver.get(search_url+malware_name)
    html = driver.page_source
    driver.close()
    soup = BeautifulSoup(html,'html.parser')
    list_parent = soup.find_all('div',class_='item-subject-title')
    links = []
    for blog_link in list_parent:
        links.append(blog_link.find('a')['href'])
    return links


def get_blog_content(url):
    driver = webdriver.Firefox()
    driver.get(url)
    html = driver.page_source
    driver.close()
    soup = BeautifulSoup(html, 'html.parser')
    blog_post = soup.find('div', class_ = 'article-body')
    blog_title = soup.find('h1', class_='page-title ng-binding')
    if blog_post and blog_title:
        return[blog_title.text.strip(),blog_post.text.strip()]
    else:
        return [None,None]

if __name__ == '__main__':
    malware = sys.argv[1]
    print(malware)
    links = get_blog_lists(malware)

    print(links)
    data =[['title','body']]

    for link in links:
        data.append(get_blog_content(symantec_url+link))

    myFile = open(malware + '.csv', 'w+')
    with myFile:
        writer = csv.writer(myFile)
        writer.writerows(data)