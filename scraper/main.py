#import requests
import urllib.request
from bs4 import BeautifulSoup
import csv
import requests
import sys
import os
symantec_url = 'https://www.symantec.com'
search_url = 'https://www.symantec.com/connect/search/'
fireeye_search_url = 'https://www.fireeye.com/search.html'
sw_search_url = 'https://www.securityweek.com/search/google/'
google_search = 'https://www.google.com/search/'
fireye_base_url = 'https://www.fireeye.com'

def get_fireye_blogs(malware_name):
    params = {'q':malware_name}
    html = requests.get(fireeye_search_url,params)
    soup = BeautifulSoup(html.content, 'html.parser')
    list_urls  = soup.find_all('a',class_ ='a03_link')
    list  =[]
    for url in list_urls:
        list.append(url['href'])
    return list

def get_fireeye_blog_content(url):
    html = requests.get(fireye_base_url+url)
    soup = BeautifulSoup(html.content,'html.parser')
    blog_body_paras = soup.find('div',class_='c00 c00v0').find_all('p')
    blog_title = soup.find('h1').text
    paras = []
    for para in blog_body_paras:
        paras.append(para.text)
    return [blog_title.strip(),' '.join(paras).strip()]

def get_sw_blog_content(url):
    html = requests.get(url)
    soup = BeautifulSoup(html.content,'html.parser')
    blog_title = ''
    blog_body = ''
    if soup.find('h2',class_='page-title'):
        blog_title = str(soup.find('h2',class_='page-title').string)
    if soup.find('div',class_='meta'):
        blog_body += soup.find('div', class_='meta').text.split('(function()')[0].strip()+'.'
    if soup.find('div', class_='content clear-block'):
        blog_body = blog_body +soup.find('div', class_='content clear-block').text.split('(function()')[0]
    return [blog_title.strip(), blog_body.strip()]

def get_securityweek_blogs(malware_name):
    params = {'q':'site%3Asecurityweek.com+'+malware_name}
    html = requests.get('https://www.google.com/search?q=site%3Asecurityweek.com+'+malware_name)
    soup = BeautifulSoup(html.content, 'html.parser')
    list_urls = soup.find_all('h3')
    list =[]
    for url in list_urls:
        sw_url  = url.next['href'][7:]
        if(not sw_url.startswith('www.google.com')):
            list.append(sw_url)
    return list




if __name__ == '__main__':

    dirname,_ = os.path.split(os.path.realpath(__file__))
    malware = sys.argv[1]
    print(malware)
    links = get_fireye_blogs(malware)
    data = [['title', 'body']]
    links = get_securityweek_blogs(malware)
    for link in set(links):
        if not get_sw_blog_content(link)[0] or not get_sw_blog_content(link)[0]:
            continue
        data.append(get_sw_blog_content(link))
    links = get_fireye_blogs(malware)
    for link in set(links):
        if not get_fireeye_blog_content(link)[0] or not get_fireeye_blog_content(link)[0]:
            continue
        data.append(get_fireeye_blog_content(link))
    myFile = open(os.path.join(os.path.join(dirname, os.pardir),'data',malware + '.csv'), 'w+',newline='',encoding='UTF-8')
    with myFile:
        writer = csv.writer(myFile)
        writer.writerows(data)